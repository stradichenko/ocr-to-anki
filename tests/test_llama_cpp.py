"""
Test llama.cpp server with Gemma 3 4B Vision-Language Model.
Verifies fully offline local inference works correctly with both text and images.
Includes detailed debugging information for performance analysis.
"""

import sys
import time
import base64
import json
from pathlib import Path
from PIL import Image
import io
from typing import Dict, Any
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from llama_cpp_server import LlamaCppServer


class TestMetrics:
    """Track test metrics and statistics."""
    
    def __init__(self):
        self.tests = []
        self.start_time = time.time()
    
    def add_test(self, test_data: Dict[str, Any]):
        """Add test result to metrics."""
        self.tests.append(test_data)
    
    def print_summary(self):
        """Print comprehensive test summary."""
        total_time = time.time() - self.start_time
        
        print("\n" + "=" * 70)
        print("PERFORMANCE SUMMARY")
        print("=" * 70)
        
        for i, test in enumerate(self.tests, 1):
            print(f"\n{i}. {test['name']}")
            print(f"   Time: {test['elapsed']:.2f}s")
            
            if 'tokens' in test:
                tokens = test['tokens']
                print(f"   Tokens:")
                if tokens.get('prompt'):
                    print(f"     - Prompt: {tokens['prompt']} tokens")
                if tokens.get('completion'):
                    print(f"     - Completion: {tokens['completion']} tokens")
                if tokens.get('total'):
                    print(f"     - Total: {tokens['total']} tokens")
                
                # Calculate tokens per second
                if tokens.get('completion') and test['elapsed'] > 0:
                    tps = tokens['completion'] / test['elapsed']
                    print(f"   Speed: {tps:.2f} tokens/sec")
            
            if 'response_info' in test:
                info = test['response_info']
                print(f"   Response:")
                if info.get('type'):
                    print(f"     - Type: {info['type']}")
                if info.get('length'):
                    print(f"     - Length: {info['length']} chars")
                if info.get('word_count'):
                    print(f"     - Words: {info['word_count']}")
        
        print(f"\nTotal test time: {total_time:.2f}s")
        print(f"Tests run: {len(self.tests)}")
        
        # Average stats
        if self.tests:
            avg_time = sum(t['elapsed'] for t in self.tests) / len(self.tests)
            print(f"Average test time: {avg_time:.2f}s")
            
            # Token statistics
            total_tokens = sum(
                t.get('tokens', {}).get('completion', 0) 
                for t in self.tests
            )
            if total_tokens > 0:
                total_gen_time = sum(
                    t['elapsed'] for t in self.tests 
                    if t.get('tokens', {}).get('completion', 0) > 0
                )
                if total_gen_time > 0:
                    avg_tps = total_tokens / total_gen_time
                    print(f"Average tokens/sec: {avg_tps:.2f}")


def analyze_response(result: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze response structure and content."""
    response_info = {}
    
    # Check response type
    content = result.get('content', '')
    response_info['type'] = 'text'
    response_info['length'] = len(content)
    response_info['word_count'] = len(content.split())
    
    # Check for special fields
    if 'thinking' in result and result['thinking']:
        response_info['has_thinking'] = True
        response_info['thinking_length'] = len(result['thinking'])
    else:
        response_info['has_thinking'] = False
    
    if 'reasoning' in result and result['reasoning']:
        response_info['has_reasoning'] = True
        response_info['reasoning_length'] = len(result['reasoning'])
    else:
        response_info['has_reasoning'] = False
    
    # Check if response looks like JSON
    try:
        json.loads(content)
        response_info['format'] = 'json'
    except:
        response_info['format'] = 'plain_text'
    
    return response_info


def extract_token_info(result: Dict[str, Any]) -> Dict[str, int]:
    """Extract token usage information from response."""
    tokens = {}
    
    # llama.cpp returns token counts in these fields
    tokens['prompt'] = result.get('prompt_tokens', 0) or result.get('tokens_evaluated', 0)
    tokens['completion'] = result.get('completion_tokens', 0) or result.get('tokens_predicted', 0)
    tokens['total'] = tokens['prompt'] + tokens['completion']
    
    return tokens


def encode_image(image_path: str, max_width: int = 800) -> str:
    """Encode image to base64, resizing if needed."""
    with Image.open(image_path) as img:
        if img.size[0] > max_width:
            ratio = max_width / img.size[0]
            new_size = (max_width, int(img.size[1] * ratio))
            img = img.resize(new_size, Image.Resampling.LANCZOS)
        
        if img.mode in ('RGBA', 'LA', 'P'):
            rgb_img = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode in ('RGBA', 'LA'):
                rgb_img.paste(img, mask=img.split()[-1])
            else:
                rgb_img.paste(img)
            img = rgb_img
        
        buffer = io.BytesIO()
        img.save(buffer, format='JPEG', quality=85)
        buffer.seek(0)
        return base64.b64encode(buffer.read()).decode('utf-8')


def test_basic_generation(metrics: TestMetrics):
    """Test basic text generation."""
    print("=== Test 1: Basic Text Generation ===\n")
    
    with LlamaCppServer(verbose=False) as server:
        prompt = "What is the capital of France?"
        
        print(f"Prompt: {prompt}")
        print(f"Prompt length: {len(prompt)} chars (~{len(prompt.split())} words)")
        print("Generating...")
        
        start = time.time()
        result = server.generate(
            prompt=prompt,
            max_tokens=50,
            temperature=0.1
        )
        elapsed = time.time() - start
        
        # Analyze response
        tokens = extract_token_info(result)
        response_info = analyze_response(result)
        
        print(f"\n‚úì Generated in {elapsed:.2f}s")
        print(f"Response: {result['content']}")
        print(f"\nDebug Info:")
        print(f"  Tokens: prompt={tokens.get('prompt', 'N/A')}, completion={tokens.get('completion', 'N/A')}, total={tokens.get('total', 'N/A')}")
        print(f"  Response type: {response_info['type']}, format: {response_info['format']}")
        print(f"  Response length: {response_info['length']} chars, {response_info['word_count']} words")
        if response_info['has_thinking']:
            print(f"  Thinking output: {response_info['thinking_length']} chars")
        print()
        
        # Record metrics
        metrics.add_test({
            'name': 'Basic Text Generation',
            'elapsed': elapsed,
            'tokens': tokens,
            'response_info': response_info,
            'prompt_length': len(prompt)
        })


def test_image_ocr(metrics: TestMetrics):
    """Test OCR on actual handwritten image with Gemma 3 vision."""
    print("=== Test 2: Image OCR with Gemma 3 Vision ===\n")
    
    # Find handwritten.jpeg
    test_dir = Path(__file__).parent
    project_root = test_dir.parent
    
    possible_paths = [
        project_root / "data" / "images" / "handwritten.jpeg",
        project_root / "tests" / "handwritten.jpeg",
        project_root / "handwritten.jpeg",
    ]
    
    image_path = None
    for path in possible_paths:
        if path.exists():
            image_path = str(path)
            break
    
    if not image_path:
        print("‚ö†Ô∏è  handwritten.jpeg not found, skipping image OCR test")
        print("   Tried locations:")
        for path in possible_paths:
            print(f"     - {path}")
        print()
        return
    
    print(f"Image: {image_path}")
    print(f"Using Gemma 3 4B Vision-Language Model")
    print()
    
    with LlamaCppServer(verbose=False) as server:
        # Check if server has vision support
        if not server.has_vision:
            print("‚ö†Ô∏è  Vision projector not found!")
            print(f"   Expected: {server.mmproj_path}")
            print("   Re-run setup: ./scripts/setup-llama-cpp.sh")
            print()
            return
        
        print(f"‚úì Vision enabled: {server.mmproj_path.name}")
        print()
        
        # Encode image (Gemma 3 uses 896x896)
        print("Encoding image...")
        image_base64 = encode_image(image_path, max_width=896)
        print(f"Image encoded: {len(image_base64) / 1024:.1f}KB")
        print()
        
        # Simple conversational OCR prompt
        prompt = "Can you do OCR of this? Just list what text you see."
        
        print(f"Prompt: {prompt}")
        print("Processing with vision model...")
        print("‚ö†Ô∏è  Note: Vision processing can take 30-90 seconds on CPU")
        print()
        
        start = time.time()
        try:
            result = server.generate(
                prompt=prompt,
                image_data=image_base64,
                max_tokens=512,
                temperature=0.1,
                timeout=900  # 15 minutes timeout for vision
            )
        except TimeoutError as e:
            print(f"\n‚ùå Vision request timed out: {e}")
            print()
            print("Possible solutions:")
            print("  1. Reduce image size (currently using 896x896)")
            print("  2. Enable GPU acceleration (set n_gpu_layers > 0)")
            print("  3. Use Tesseract for OCR instead of vision model")
            print()
            return
        
        elapsed = time.time() - start
        
        # Analyze response
        tokens = extract_token_info(result)
        response_info = analyze_response(result)
        
        print(f"\n‚úì Generated in {elapsed:.2f}s")
        print(f"OCR Result:\n{result['content']}")
        print(f"\nDebug Info:")
        print(f"  Tokens: prompt={tokens.get('prompt', 'N/A')}, completion={tokens.get('completion', 'N/A')}, total={tokens.get('total', 'N/A')}")
        print(f"  Response type: {response_info['type']}, format: {response_info['format']}")
        print(f"  Response length: {response_info['length']} chars, {response_info['word_count']} words")
        if tokens.get('completion') and elapsed > 0:
            print(f"  Generation speed: {tokens['completion']/elapsed:.2f} tokens/sec")
        print()
        
        # Record metrics
        metrics.add_test({
            'name': 'Image OCR (Vision)',
            'elapsed': elapsed,
            'tokens': tokens,
            'response_info': response_info,
            'prompt_length': len(prompt),
            'has_image': True
        })


def test_vocabulary_ocr_prompt(metrics: TestMetrics):
    """Test OCR-style prompt for vocabulary extraction (text simulation)."""
    print("=== Test 3: Vocabulary Extraction Prompt ===\n")
    
    with LlamaCppServer(verbose=False) as server:
        # Simulate OCR scenario with text description
        prompt = """I have a handwritten French vocabulary list with these words visible:
- bonjour
- merci
- au revoir
- s'il vous pla√Æt

Please extract these words and list each one on a separate line."""
        
        print(f"Prompt: (simulated OCR scenario)")
        print(f"Prompt length: {len(prompt)} chars (~{len(prompt.split())} words)")
        print("Generating...")
        
        start = time.time()
        result = server.generate(
            prompt=prompt,
            max_tokens=256,
            temperature=0.1,
            stop=["\n\n\n", "</s>", "---"]
        )
        elapsed = time.time() - start
        
        # Analyze response
        tokens = extract_token_info(result)
        response_info = analyze_response(result)
        
        print(f"\n‚úì Generated in {elapsed:.2f}s")
        print(f"Response:\n{result['content']}")
        print(f"\nDebug Info:")
        print(f"  Tokens: prompt={tokens.get('prompt', 'N/A')}, completion={tokens.get('completion', 'N/A')}, total={tokens.get('total', 'N/A')}")
        print(f"  Response type: {response_info['type']}, format: {response_info['format']}")
        print(f"  Response length: {response_info['length']} chars, {response_info['word_count']} words")
        if tokens.get('completion') and elapsed > 0:
            print(f"  Generation speed: {tokens['completion']/elapsed:.2f} tokens/sec")
        print()
        
        # Record metrics
        metrics.add_test({
            'name': 'Vocabulary Extraction Prompt',
            'elapsed': elapsed,
            'tokens': tokens,
            'response_info': response_info,
            'prompt_length': len(prompt)
        })


def test_definition_generation(metrics: TestMetrics):
    """Test definition generation for vocabulary."""
    print("=== Test 4: Vocabulary Definition ===\n")
    
    with LlamaCppServer(verbose=False) as server:
        word = "bonjour"
        prompt = f"""Hey! I'm learning vocabulary and need your help. Can you give me a clear, simple definition of the word "{word}" in English?

Just the definition please, nothing else. Keep it concise but informative, like a dictionary entry."""
        
        print(f"Word: {word}")
        print(f"Prompt length: {len(prompt)} chars (~{len(prompt.split())} words)")
        print("Generating definition...")
        
        start = time.time()
        result = server.generate(
            prompt=prompt,
            max_tokens=150,
            temperature=0.2
        )
        elapsed = time.time() - start
        
        # Analyze response
        tokens = extract_token_info(result)
        response_info = analyze_response(result)
        
        print(f"\n‚úì Generated in {elapsed:.2f}s")
        print(f"Definition: {result['content']}")
        print(f"\nDebug Info:")
        print(f"  Tokens: prompt={tokens.get('prompt', 'N/A')}, completion={tokens.get('completion', 'N/A')}, total={tokens.get('total', 'N/A')}")
        print(f"  Response type: {response_info['type']}, format: {response_info['format']}")
        print(f"  Response length: {response_info['length']} chars, {response_info['word_count']} words")
        if tokens.get('completion') and elapsed > 0:
            print(f"  Generation speed: {tokens['completion']/elapsed:.2f} tokens/sec")
        print()
        
        # Record metrics
        metrics.add_test({
            'name': 'Vocabulary Definition',
            'elapsed': elapsed,
            'tokens': tokens,
            'response_info': response_info,
            'prompt_length': len(prompt)
        })


def test_example_generation(metrics: TestMetrics):
    """Test example sentence generation."""
    print("=== Test 5: Example Sentences ===\n")
    
    with LlamaCppServer(verbose=False) as server:
        word = "bonjour"
        prompt = f"""I'm studying the word "{word}" and need some help with examples. Could you create 2 simple, natural sentences using this word? The sentences should be in French.

Please write just the 2 sentences, one per line."""
        
        print(f"Word: {word}")
        print(f"Prompt length: {len(prompt)} chars (~{len(prompt.split())} words)")
        print("Generating examples...")
        
        start = time.time()
        result = server.generate(
            prompt=prompt,
            max_tokens=200,
            temperature=0.7
        )
        elapsed = time.time() - start
        
        # Analyze response
        tokens = extract_token_info(result)
        response_info = analyze_response(result)
        
        print(f"\n‚úì Generated in {elapsed:.2f}s")
        print(f"Examples:\n{result['content']}")
        print(f"\nDebug Info:")
        print(f"  Tokens: prompt={tokens.get('prompt', 'N/A')}, completion={tokens.get('completion', 'N/A')}, total={tokens.get('total', 'N/A')}")
        print(f"  Response type: {response_info['type']}, format: {response_info['format']}")
        print(f"  Response length: {response_info['length']} chars, {response_info['word_count']} words")
        if tokens.get('completion') and elapsed > 0:
            print(f"  Generation speed: {tokens['completion']/elapsed:.2f} tokens/sec")
        print()
        
        # Record metrics
        metrics.add_test({
            'name': 'Example Sentences',
            'elapsed': elapsed,
            'tokens': tokens,
            'response_info': response_info,
            'prompt_length': len(prompt)
        })


def main():
    """Run all tests."""
    print("=" * 70)
    print("llama.cpp Gemma 3 4B Vision-Language Model Test Suite")
    print(f"Test started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print()
    
    metrics = TestMetrics()
    
    try:
        # Check if model exists
        server = LlamaCppServer(verbose=False)
        print(f"‚úì Model: {server.model_path.name}")
        print(f"  Size: {server.model_path.stat().st_size / (1024**3):.2f} GB")
        print(f"  Type: {server.model_type}")
        
        if server.has_vision:
            print(f"‚úì Vision Projector: {server.mmproj_path.name}")
            print(f"  Size: {server.mmproj_path.stat().st_size / (1024**3):.2f} GB")
            print(f"  Capabilities: Text + Image understanding")
        else:
            print(f"‚úó Vision: Not available")
            print(f"  Expected projector at: {server.mmproj_path}")
        
        print(f"\nServer Configuration:")
        print(f"  Host: {server.host}:{server.port}")
        print(f"  Context size: {server.context_size}")
        print(f"  GPU layers: {server.n_gpu_layers}")
        print()
        
        # Run tests
        test_basic_generation(metrics)
        test_image_ocr(metrics)  # Now actually uses vision!
        test_vocabulary_ocr_prompt(metrics)
        test_definition_generation(metrics)
        test_example_generation(metrics)
        
        # Print comprehensive summary
        metrics.print_summary()
        
        print("\n" + "=" * 70)
        print("‚úÖ All tests completed!")
        print("=" * 70)
        print()
        print("Model Capabilities:")
        print("  ‚úì Text generation (definitions, examples, translations)")
        print("  ‚úì Image understanding (OCR, visual question answering)")
        print("  ‚úì Multi-turn conversations")
        print("  ‚úì 128K context window (8K output)")
        print()
        print("Recommended Pipeline:")
        print("  1. Image ‚Üí Gemma 3 Vision ‚Üí Extracted text")
        print("  2. Text ‚Üí Gemma 3 ‚Üí Definitions + Examples")
        print("  3. Enriched data ‚Üí Anki flashcards")
        print()
        print("Performance Notes:")
        if server.has_vision:
            print("  ‚Ä¢ Vision tasks: 5-15s per image (first run)")
            print("  ‚Ä¢ Text tasks: 1-3s per request")
            print("  ‚Ä¢ Batch processing recommended for multiple images")
        print()
        
        # Save detailed metrics to file
        metrics_file = Path(__file__).parent / f"test_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(metrics_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'model': {
                    'path': str(server.model_path),
                    'size_gb': server.model_path.stat().st_size / (1024**3),
                    'type': server.model_type,
                    'has_vision': server.has_vision
                },
                'vision_projector': {
                    'path': str(server.mmproj_path) if server.mmproj_path else None,
                    'size_gb': server.mmproj_path.stat().st_size / (1024**3) if server.has_vision else None
                } if server.has_vision else None,
                'tests': metrics.tests,
                'total_time': time.time() - metrics.start_time
            }, f, indent=2)
        
        print(f"üìä Detailed metrics saved to: {metrics_file}")
        print()
    
    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print()
        print("Run the setup script first:")
        print("  ./scripts/setup-llama-cpp.sh")
        sys.exit(1)
    
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
